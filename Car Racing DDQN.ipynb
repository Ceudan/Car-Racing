{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ASmSgEVBLvc3"},"outputs":[],"source":["# You will need to run this block twice to make it effective\n","!apt-get update \n","!apt-get install cmake \n","!pip install --upgrade setuptools \n","!pip install ez_setup \n","!pip install gym==0.24.1\n","!pip install gym[all]\n","\n","!pip install gym pyvirtualdisplay \n","!apt-get install -y xvfb python-opengl ffmpeg "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BReeWH_OdUkb"},"outputs":[],"source":["from importlib import reload\n","import utils\n","reload(utils)\n","from utils import*\n","\n","import gym\n","from gym.wrappers.monitoring import video_recorder\n","from gym.wrappers.monitoring.video_recorder import VideoRecorder\n","from gym.wrappers.record_video import RecordVideo\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","from pyvirtualdisplay import Display\n","from IPython import display as ipythondisplay\n","\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","from torch import nn\n","import copy\n","from collections import deque\n","import random\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import math\n","\n","from torch import randint\n","from time import sleep\n","import pickle\n","import statistics as st\n","from gym.core import RewardWrapper\n","import gc\n","\n","\n","\n","display = Display(visible=0, size=(1400, 900))\n","display.start()\n","\n","\"\"\"\n","Utility functions to enable video recording of gym environment \n","and displaying it.\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","\n","def simulate(agent=None,env=None,epsilon=0,memory=None,render=False):\n","  agent.eval()\n","  env.reset()\n","  if(render):\n","    env.env = RecordVideo(env.env, './video')\n","    env.env.render()\n","  state,rew,done,info = env.skip_episodes(70,[0,0.1,0])\n","  ep_len = 0\n","  while not done:\n","      # exploitation(0) vs exploration(1)\n","      sample = torch.bernoulli(torch.tensor(epsilon).float())\n","      if(sample==1):\n","        A = torch.randint(0,3,(1,))\n","      else:\n","        A = agent.get_action(state)\n","\n","      # progress a time step\n","      next_state, rew, done, info = env.step(agent.convert_action(A,state))\n","      #plot_image(state)\n","      #print(rew)\n","\n","      if(done):\n","        break\n","\n","      # collect memory\n","      if(memory!=None):\n","        memory.collect([state, A, rew, next_state])\n","      state = next_state\n","\n","      ep_len = env.ep_len\n","      # stop criteria \n","      if(ep_len>2000):\n","        break\n","\n","  # readd 100 to episode reward to resync measured reward with documentation (undo the -100 penalty)\n","  score = env.real_rew\n","  if(render):\n","    print(\"score\",score,\"ep_len\",ep_len)\n","    env.env.close()\n","    show_video()\n","  \n","  return score,ep_len\n","\n","\n","def test_model(agent, env, episodes=1):\n","  rewards = []\n","  ep_lens = []\n","  for i in range(0,episodes):\n","    rew,ep_len = simulate(agent,env)\n","    rewards.append(rew)\n","    ep_lens.append(ep_len)\n","    print(\"Test \"+str(i+1)+\"/\"+str(episodes)+\": reward =\",rew,\" episode len =\",ep_len)\n","  print(\"\\nAverage Reward = \",sum(rewards)/len(rewards),\"Average Ep_len = \",sum(ep_lens)/len(ep_lens),\"\\n\")\n","  return rewards,ep_lens\n","\n","\n","class ExperienceReplay(object):\n","# one entry is [state,action,reward,next_state]\n","  def  __init__(self, length):\n","    self.experience_replay = deque(maxlen=length)\n","  def collect(self,experience):\n","    self.experience_replay.append(experience)\n","    return\n","  def sample_from_experience(self, sample_size):\n","    sample_size = min(sample_size,len(self.experience_replay))\n","    sample = random.sample(self.experience_replay,sample_size)\n","    state = torch.tensor([episode[0] for episode in sample]).float()\n","    action = torch.tensor([episode[1] for episode in sample]).float()\n","    reward = torch.tensor([episode[2] for episode in sample]).float()\n","    next_state = torch.tensor([episode[3] for episode in sample]).float()\n","\n","    return state,action,reward,next_state\n","\n","\n","\n","class DQN_Network(nn.Module):\n","  def __init__(self,gamma = None):\n","    super().__init__()\n","    #layers\n","    self.LeakyReLU = nn.LeakyReLU()\n","    self.conv1 = nn.Conv2d(1,8,kernel_size = 7, stride = 4,padding = 0)\n","    self.conv2 = nn.Conv2d(8,16,kernel_size = 3, stride = 1,padding = 2)\n","    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","    self.fc1 = nn.Linear(577,256)\n","    self.fc2 = nn.Linear(256,50)\n","    self.fc3 = nn.Linear(50,3)\n","    self.batchnormCNN1 = nn.BatchNorm2d(num_features = 8)\n","    self.batchnormCNN2 = nn.BatchNorm2d(num_features = 16)\n","    self.batchnormFC1 = nn.BatchNorm1d(num_features = 256)\n","    self.flatten = nn.Flatten()\n","    self.gamma = gamma\n","  def forward(self,x):\n","    # reformat image (input = BS,96,96, or 96,96) (output = BS,1,96,96)\n","    x = torch.from_numpy(np.ascontiguousarray(x)).float()\n","    if(x.dim()==2):\n","      x = torch.unsqueeze(x,dim=0)\n","      x = torch.unsqueeze(x,dim=0)\n","    elif(x.dim()==3):\n","      x = torch.unsqueeze(x,dim=1)\n","    subimage = (x[:,:,84:96,13:14]-0.495)*10\n","    speed = torch.sum(subimage,dim=(2,3))\n","    x = x[:,:,:84,:]\n","    #plot_image(np.squeeze(x.detach().numpy()))\n","    \n","    #print(x.shape)\n","    x = self.batchnormCNN1(self.LeakyReLU(self.conv1(x)))\n","    #print(x.shape)\n","    x = self.pool(x)\n","    #print(x.shape)\n","    x = self.batchnormCNN2(self.LeakyReLU(self.conv2(x)))\n","    #print(x.shape)\n","    x = self.pool(x)\n","    #print(x.shape)\n","    x = self.flatten(x)\n","    #print(x.shape)\n","    x = torch.cat((x,speed),dim=1)\n","    x = self.batchnormFC1(self.LeakyReLU(self.fc1(x)))\n","    #x = self.LeakyReLU(self.fc1(x))\n","    #print(x.shape)\n","    x = self.LeakyReLU(self.fc2(x))\n","    #print(x.shape)\n","    x = self.fc3(x) \n","    #print(x.shape)\n","    return x\n","  def get_action(self,state):\n","    qvals = self.forward(state)\n","    return torch.argmax(qvals,1) \n","  def convert_action(self,action,state):\n","    # determine if you are going too fast\n","    speed = get_speed(state).item()\n","    if(speed>3.5):\n","      accel = 0\n","    elif(speed>2.5):\n","      accel = 0\n","    else:\n","      accel = 0.1\n","    # convert action from index, to a list of turning,engine,breaking strengths\n","    action = action.item()\n","    # Discretized action space (left-forward,straight-forward,right-forward)\n","    if(action == 0):\n","      return [-0.3,accel,0]\n","    elif(action == 1):\n","      return [0,accel,0]\n","    elif(action == 2):\n","      return [0.3,accel,0]\n"]},{"cell_type":"markdown","source":["### Training"],"metadata":{"id":"arykaWSyIXJ0"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"GkB_3IPIEK5l","executionInfo":{"status":"ok","timestamp":1655784113026,"user_tz":240,"elapsed":228,"user":{"displayName":"tigerfan707","userId":"17162744304693628162"}}},"outputs":[],"source":["def load_memory(new,epsilon,exp_replay_size,initial_size=None):\n","  if(initial_size==None):\n","    initial_size=exp_replay_size\n","  # Create the model\n","  env = wrap_env(gym.make(\"CarRacing-v1\").unwrapped)\n","  agent = DQN_Network()\n","  if(not new):\n","    agent.load_state_dict(torch.load(\"car-racing-dqn.pth\"))\n","  memory = ExperienceReplay(exp_replay_size)\n","\n","  # initiliaze experience replay\n","  index = 0\n","  for i in range(exp_replay_size):\n","      state = env.reset()\n","      simulate(agent,env,epsilon = epsilon, memory = memory)\n","      if(len(memory.experience_replay)>=initial_size):\n","        break\n","      print(len(memory.experience_replay))\n","\n","  return memory\n","\n","\n","def update(agent,optimizer,loss_func,target_agent,memory,batch_size):\n","  agent.train()\n","  target_agent.eval()\n","  # current (S,A) Qval\n","  state,action,reward,next_state = memory.sample_from_experience(batch_size)\n","  Qvals = agent(state)\n","  curr_Qval = Qvals[torch.arange(Qvals.size(0)),action.long()]\n","  \n","  # best next (S,A) Qval\n","  with torch.no_grad():\n","    next_Qval, indices = torch.max(target_agent(next_state),dim=1)\n","\n","  # update agent\n","  #print(reward + agent.gamma*next_Qval,curr_Qval)\n","  loss = loss_func(reward + agent.gamma*next_Qval, curr_Qval)\n","  loss.backward(retain_graph = False)\n","  optimizer.step()\n","  optimizer.zero_grad()\n","\n","\n","def train(new,num_ep,lr_start,epsilon_start,gamma,memory):\n","  # set hyperparamters\n","  agent = DQN_Network(gamma=gamma)\n"," \n","  # start new run\n","  if(new):\n","    reward_hist = []; ep_len_hist = []; lr_hist = []; epsilon_hist = []\n","  # load previous runs\n","  else:\n","    agent.load_state_dict(torch.load(\"car-racing-dqn.pth\")); reward_hist = load_list(\"reward_hist.data\");ep_len_hist = load_list(\"ep_len_hist.data\");epsilon_hist = load_list(\"epsilon_hist.data\");lr_hist = load_list(\"lr_hist.data\")\n","\n","  #initialize models\n","  target_agent = DQN_Network(agent.gamma)\n","  target_agent.load_state_dict(agent.state_dict())\n","  env = wrap_env(gym.make(\"CarRacing-v1\").unwrapped)\n","  optimizer = torch.optim.SGD(agent.parameters(),lr_start)\n","  MSELoss = torch.nn.MSELoss()\n","\n","  # training loop\n","  for ep_num in tqdm(range(0,num_ep)):\n","    lr = lr_start*(0.99042**ep_num)\n","    epsilon = epsilon_start*(0.99424**ep_num)\n","\n","    for param_group in optimizer.param_groups:\n","      param_group['lr'] = lr\n","\n","    state, done, losses, ep_len, reward = env.reset(), False, 0, 0, 0\n","    reward,ep_len = simulate(agent,env,epsilon = epsilon, memory = memory)       \n","  \n","    for i in range(0,30):\n","      update(agent,optimizer,MSELoss,target_agent,memory,batch_size=32)\n","    target_agent.load_state_dict(agent.state_dict())\n","    gc.collect(generation=2)\n","\n","    if(ep_num%3==0):\n","      reward, ep_len = test_model(agent=agent,env=env, episodes=1)\n","      print(\"Settings: lr =\",lr,\"epsilon =\",epsilon)\n","      print(\"Test Result: reward =\",reward[0],\"episode length =\",ep_len[0])\n","      reward_hist.append(reward[0])\n","      ep_len_hist.append(ep_len[0])\n","      lr_hist.append(lr)\n","      epsilon_hist.append(epsilon)\n","    \n","    if(ep_num%30==0):\n","      # save results\n","      torch.save(agent.state_dict(),\"car-racing-dqn.pth\");save_list(reward_hist,\"reward_hist.data\");save_list(ep_len_hist,\"ep_len_hist.data\");save_list(epsilon_hist,\"epsilon_hist.data\");save_list(lr_hist,\"lr_hist.data\")"]},{"cell_type":"code","source":["memory = load_memory(new=True,epsilon=1,exp_replay_size=2000)\n","train(new=True,num_ep = 360,lr_start=0.0003,epsilon_start=0.8,gamma=0.92,memory=memory)"],"metadata":{"id":"_ijJZqDaVqEr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Go4nqY0dtXJ"},"outputs":[],"source":["env = wrap_env(gym.make(\"CarRacing-v1\").unwrapped)\n","agent = DQN_Network()\n","agent.load_state_dict(torch.load(\"car-racing-dqn.pth\"))\n","\n","simulate(agent=agent,env=env,render=True)\n","#test_model(agent,env,episodes=10)"]},{"cell_type":"code","source":["print(\"lr_hist\")\n","hist = load_list(\"lr_hist.data\")\n","for stage in hist:\n","  print(stage)\n","print(\"\\nepsilon_hist\")\n","hist = load_list(\"epsilon_hist.data\")\n","for stage in hist:\n","  print(stage)\n","print(\"\\nep_len_hist\")\n","hist = load_list(\"ep_len_hist.data\")\n","for stage in hist:\n","  print(st.mean(stage))\n","print(\"\\nreward_hist\")\n","hist = load_list(\"reward_hist.data\")\n","for stage in hist:\n","  print(st.mean(stage))"],"metadata":{"id":"JUIN0LDh0n3B"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Car Racing DDQN.ipynb","provenance":[],"authorship_tag":"ABX9TyMuS3RwLfeD7aadel4YH0VM"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}