{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ASmSgEVBLvc3"},"outputs":[],"source":["# You will need to run this block twice to make it effective\n","!apt-get update \n","!apt-get install cmake \n","!pip install --upgrade setuptools \n","!pip install ez_setup \n","!pip install gym==0.24.1\n","!pip install gym[all]\n","\n","!pip install gym pyvirtualdisplay \n","!apt-get install -y xvfb python-opengl ffmpeg "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BReeWH_OdUkb"},"outputs":[],"source":["from importlib import reload\n","import utils\n","reload(utils)\n","from utils import*\n","\n","import gym\n","from gym.wrappers.monitoring import video_recorder\n","from gym.wrappers.monitoring.video_recorder import VideoRecorder\n","from gym.wrappers.record_video import RecordVideo\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","from pyvirtualdisplay import Display\n","from IPython import display as ipythondisplay\n","\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","from torch import nn\n","import copy\n","from collections import deque\n","import random\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import math\n","\n","from torch import randint\n","from time import sleep\n","import pickle\n","import statistics as st\n","from gym.core import RewardWrapper\n","import gc\n","\n","\n","\n","display = Display(visible=0, size=(1400, 900))\n","display.start()\n","\n","\"\"\"\n","Utility functions to enable video recording of gym environment \n","and displaying it.\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","\n","def simulate(agent=None,env=None,epsilon=0,memory=None,render=False):\n","  agent.eval()\n","  env.reset()\n","  if(render):\n","    env.env = RecordVideo(env.env, './video')\n","    env.env.render()\n","  state,rew,done,info = env.skip_episodes(70,[0,0.1,0])\n","  ep_len = 0\n","  while not done:\n","      # exploitation(0) vs exploration(1)\n","      sample = torch.bernoulli(torch.tensor(epsilon).float())\n","      if(sample==1):\n","        A = torch.randint(0,3,(1,))\n","      else:\n","        A = agent.get_action(state)\n","\n","      # progress a time step\n","      next_state, rew, done, info = env.step(agent.convert_action(A,state))\n","      #plot_image(state)\n","      #print(rew)\n","\n","      if(done):\n","        break\n","\n","      # collect memory\n","      if(memory!=None):\n","        memory.collect([state, A, rew, next_state])\n","      state = next_state\n","\n","      ep_len = env.ep_len\n","      # stop criteria \n","      if(ep_len>2000):\n","        break\n","\n","  # readd 100 to episode reward to resync measured reward with documentation (undo the -100 penalty)\n","  score = env.real_rew\n","  if(render):\n","    print(\"score\",score,\"ep_len\",ep_len)\n","    env.env.close()\n","    show_video()\n","  \n","  return score,ep_len\n","\n","\n","def test_model(agent, env, episodes=1):\n","  rewards = []\n","  ep_lens = []\n","  for i in range(0,episodes):\n","    rew,ep_len = simulate(agent,env)\n","    rewards.append(rew)\n","    ep_lens.append(ep_len)\n","    print(\"Test \"+str(i+1)+\"/\"+str(episodes)+\": reward =\",rew,\" episode len =\",ep_len)\n","  print(\"\\nAverage Reward = \",sum(rewards)/len(rewards),\"Average Ep_len = \",sum(ep_lens)/len(ep_lens),\"\\n\")\n","  return rewards,ep_lens\n","\n","\n","class ExperienceReplay(object):\n","# one entry is [state,action,reward,next_state]\n","  def  __init__(self, length):\n","    self.experience_replay = deque(maxlen=length)\n","  def collect(self,experience):\n","    self.experience_replay.append(experience)\n","    return\n","  def sample_from_experience(self, sample_size):\n","    sample_size = min(sample_size,len(self.experience_replay))\n","    sample = random.sample(self.experience_replay,sample_size)\n","    state = torch.tensor([episode[0] for episode in sample]).float()\n","    action = torch.tensor([episode[1] for episode in sample]).float()\n","    reward = torch.tensor([episode[2] for episode in sample]).float()\n","    next_state = torch.tensor([episode[3] for episode in sample]).float()\n","\n","    return state,action,reward,next_state\n","\n","\n","\n","class DQN_Network(nn.Module):\n","  def __init__(self,gamma = None):\n","    super().__init__()\n","    #layers\n","    self.LeakyReLU = nn.LeakyReLU()\n","    self.conv1 = nn.Conv2d(1,8,kernel_size = 7, stride = 4,padding = 0)\n","    self.conv2 = nn.Conv2d(8,16,kernel_size = 3, stride = 1,padding = 2)\n","    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","    self.fc1 = nn.Linear(577,256)\n","    self.fc2 = nn.Linear(256,50)\n","    self.fc3 = nn.Linear(50,3)\n","    self.batchnormCNN1 = nn.BatchNorm2d(num_features = 8)\n","    self.batchnormCNN2 = nn.BatchNorm2d(num_features = 16)\n","    self.batchnormFC1 = nn.BatchNorm1d(num_features = 256)\n","    self.flatten = nn.Flatten()\n","    self.gamma = gamma\n","  def forward(self,x):\n","    # reformat image (input = BS,96,96, or 96,96) (output = BS,1,96,96)\n","    x = torch.from_numpy(np.ascontiguousarray(x)).float()\n","    if(x.dim()==2):\n","      x = torch.unsqueeze(x,dim=0)\n","      x = torch.unsqueeze(x,dim=0)\n","    elif(x.dim()==3):\n","      x = torch.unsqueeze(x,dim=1)\n","    subimage = (x[:,:,84:96,13:14]-0.495)*10\n","    speed = torch.sum(subimage,dim=(2,3))\n","    x = x[:,:,:84,:]\n","    #plot_image(np.squeeze(x.detach().numpy()))\n","    \n","    #print(x.shape)\n","    x = self.batchnormCNN1(self.LeakyReLU(self.conv1(x)))\n","    #print(x.shape)\n","    x = self.pool(x)\n","    #print(x.shape)\n","    x = self.batchnormCNN2(self.LeakyReLU(self.conv2(x)))\n","    #print(x.shape)\n","    x = self.pool(x)\n","    #print(x.shape)\n","    x = self.flatten(x)\n","    #print(x.shape)\n","    x = torch.cat((x,speed),dim=1)\n","    x = self.batchnormFC1(self.LeakyReLU(self.fc1(x)))\n","    #x = self.LeakyReLU(self.fc1(x))\n","    #print(x.shape)\n","    x = self.LeakyReLU(self.fc2(x))\n","    #print(x.shape)\n","    x = self.fc3(x) \n","    #print(x.shape)\n","    return x\n","  def get_action(self,state):\n","    qvals = self.forward(state)\n","    return torch.argmax(qvals,1) \n","  def convert_action(self,action,state):\n","    # determine if you are going too fast\n","    speed = get_speed(state).item()\n","    if(speed>3.5):\n","      accel = 0\n","    elif(speed>2.5):\n","      accel = 0\n","    else:\n","      accel = 0.1\n","    # convert action from index, to a list of turning,engine,breaking strengths\n","    action = action.item()\n","    # Discretized action space (left-forward,straight-forward,right-forward)\n","    if(action == 0):\n","      return [-0.3,accel,0]\n","    elif(action == 1):\n","      return [0,accel,0]\n","    elif(action == 2):\n","      return [0.3,accel,0]\n"]},{"cell_type":"markdown","source":["### Training"],"metadata":{"id":"arykaWSyIXJ0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GkB_3IPIEK5l"},"outputs":[],"source":["def load_memory(new,epsilon,exp_replay_size,initial_size=None):\n","  if(initial_size==None):\n","    initial_size=exp_replay_size\n","  # Create the model\n","  env = wrap_env(gym.make(\"CarRacing-v1\").unwrapped)\n","  agent = DQN_Network()\n","  if(not new):\n","    agent.load_state_dict(torch.load(\"car-racing-dqn.pth\"))\n","  memory = ExperienceReplay(exp_replay_size)\n","\n","  # initiliaze experience replay\n","  index = 0\n","  for i in range(exp_replay_size):\n","      state = env.reset()\n","      simulate(agent,env,epsilon = epsilon, memory = memory)\n","      if(len(memory.experience_replay)>=initial_size):\n","        break\n","      print(len(memory.experience_replay))\n","\n","  return memory\n","\n","\n","def update(agent,optimizer,loss_func,target_agent,memory,batch_size):\n","  agent.train()\n","  target_agent.eval()\n","  # current (S,A) Qval\n","  state,action,reward,next_state = memory.sample_from_experience(batch_size)\n","  Qvals = agent(state)\n","  curr_Qval = Qvals[torch.arange(Qvals.size(0)),action.long()]\n","  \n","  # best next (S,A) Qval\n","  with torch.no_grad():\n","    next_Qval, indices = torch.max(target_agent(next_state),dim=1)\n","\n","  # update agent\n","  #print(reward + agent.gamma*next_Qval,curr_Qval)\n","  loss = loss_func(reward + agent.gamma*next_Qval, curr_Qval)\n","  loss.backward(retain_graph = False)\n","  optimizer.step()\n","  optimizer.zero_grad()\n","\n","\n","def train(new,num_ep,lr_start,epsilon_start,gamma,memory):\n","  # set hyperparamters\n","  agent = DQN_Network(gamma=gamma)\n"," \n","  # start new run\n","  if(new):\n","    reward_hist = []; ep_len_hist = []; lr_hist = []; epsilon_hist = []\n","  # load previous runs\n","  else:\n","    agent.load_state_dict(torch.load(\"car-racing-dqn.pth\")); reward_hist = load_list(\"reward_hist.data\");ep_len_hist = load_list(\"ep_len_hist.data\");epsilon_hist = load_list(\"epsilon_hist.data\");lr_hist = load_list(\"lr_hist.data\")\n","\n","  #initialize models\n","  target_agent = DQN_Network(agent.gamma)\n","  target_agent.load_state_dict(agent.state_dict())\n","  env = wrap_env(gym.make(\"CarRacing-v1\").unwrapped)\n","  optimizer = torch.optim.SGD(agent.parameters(),lr_start)\n","  MSELoss = torch.nn.MSELoss()\n","\n","  # training loop\n","  for ep_num in tqdm(range(0,num_ep)):\n","    lr = lr_start*(0.99042**ep_num)\n","    epsilon = epsilon_start*(0.99424**ep_num)\n","\n","    for param_group in optimizer.param_groups:\n","      param_group['lr'] = lr\n","\n","    state, done, losses, ep_len, reward = env.reset(), False, 0, 0, 0\n","    reward,ep_len = simulate(agent,env,epsilon = epsilon, memory = memory)       \n","  \n","    for i in range(0,30):\n","      update(agent,optimizer,MSELoss,target_agent,memory,batch_size=32)\n","    target_agent.load_state_dict(agent.state_dict())\n","    gc.collect(generation=2)\n","\n","    if(ep_num%3==0):\n","      reward, ep_len = test_model(agent=agent,env=env, episodes=1)\n","      print(\"Settings: lr =\",lr,\"epsilon =\",epsilon)\n","      print(\"Test Result: reward =\",reward[0],\"episode length =\",ep_len[0])\n","      reward_hist.append(reward[0])\n","      ep_len_hist.append(ep_len[0])\n","      lr_hist.append(lr)\n","      epsilon_hist.append(epsilon)\n","    \n","    if(ep_num%30==0):\n","      # save results\n","      torch.save(agent.state_dict(),\"car-racing-dqn.pth\");save_list(reward_hist,\"reward_hist.data\");save_list(ep_len_hist,\"ep_len_hist.data\");save_list(epsilon_hist,\"epsilon_hist.data\");save_list(lr_hist,\"lr_hist.data\")"]},{"cell_type":"code","source":["memory = load_memory(new=True,epsilon=1,exp_replay_size=2000)\n","train(new=True,num_ep = 360,lr_start=0.0003,epsilon_start=0.8,gamma=0.92,memory=memory)"],"metadata":{"id":"_ijJZqDaVqEr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655672117652,"user_tz":240,"elapsed":3040604,"user":{"displayName":"tigerfan707","userId":"17162744304693628162"}},"outputId":"28b8df13-2b08-4491-f91b-d01ce0119799"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:98: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n","  \"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\n"]},{"output_type":"stream","name":"stdout","text":["12\n","20\n","27\n","35\n","44\n","55\n","64\n","74\n","90\n","99\n","108\n","119\n","136\n","157\n","169\n","185\n","205\n","219\n","227\n","237\n","254\n","268\n","297\n","305\n","321\n","333\n","352\n","360\n","367\n","383\n","394\n","405\n","423\n","444\n","473\n","487\n","499\n","511\n","520\n","535\n","547\n","555\n","563\n","571\n","587\n","595\n","613\n","630\n","638\n","653\n","664\n","686\n","699\n","706\n","719\n","728\n","738\n","748\n","768\n","786\n","795\n","806\n","821\n","847\n","869\n","885\n","905\n","919\n","928\n","945\n","957\n","968\n","980\n","988\n","1010\n","1018\n","1029\n","1042\n","1051\n","1064\n","1083\n","1091\n","1100\n","1137\n","1164\n","1183\n","1195\n","1209\n","1225\n","1237\n","1254\n","1265\n","1276\n","1288\n","1299\n","1312\n","1329\n","1338\n","1346\n","1364\n","1377\n","1386\n","1394\n","1402\n","1411\n","1420\n","1432\n","1445\n","1457\n","1466\n","1479\n","1490\n","1501\n","1510\n","1520\n","1539\n","1562\n","1570\n","1580\n","1588\n","1597\n","1608\n","1625\n","1640\n","1667\n","1677\n","1690\n","1710\n","1733\n","1746\n","1758\n","1768\n","1777\n","1794\n","1802\n","1810\n","1824\n","1832\n","1846\n","1863\n","1878\n","1897\n","1913\n","1933\n","1945\n","1962\n","1979\n","1990\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 1/360 [00:05<35:45,  5.98s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 34.98156996587023  episode len = 125\n","\n","Average Reward =  34.98156996587023 Average Ep_len =  125.0 \n","\n","Settings: lr = 0.0003 epsilon = 0.8\n","Test Result: reward = 34.98156996587023 episode length = 125\n"]},{"output_type":"stream","name":"stderr","text":["  1%|          | 4/360 [00:20<31:12,  5.26s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 37.741516245487276  episode len = 125\n","\n","Average Reward =  37.741516245487276 Average Ep_len =  125.0 \n","\n","Settings: lr = 0.00029146033499462634 epsilon = 0.7862554733576193\n","Test Result: reward = 37.741516245487276 episode length = 125\n"]},{"output_type":"stream","name":"stderr","text":["  2%|▏         | 7/360 [00:34<29:10,  4.96s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 30.26666666666661  episode len = 110\n","\n","Average Reward =  30.26666666666661 Average Ep_len =  110.0 \n","\n","Settings: lr = 0.0002831637562505994 epsilon = 0.7727470867310173\n","Test Result: reward = 30.26666666666661 episode length = 110\n"]},{"output_type":"stream","name":"stderr","text":["  3%|▎         | 10/360 [00:49<30:18,  5.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 38.65901639344254  episode len = 135\n","\n","Average Reward =  38.65901639344254 Average Ep_len =  135.0 \n","\n","Settings: lr = 0.0002751033441837881 epsilon = 0.7594707830792716\n","Test Result: reward = 38.65901639344254 episode length = 135\n"]},{"output_type":"stream","name":"stderr","text":["  4%|▎         | 13/360 [01:03<29:01,  5.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 32.77353951890027  episode len = 115\n","\n","Average Reward =  32.77353951890027 Average Ep_len =  115.0 \n","\n","Settings: lr = 0.00026727237617982957 epsilon = 0.7464225750640932\n","Test Result: reward = 32.77353951890027 episode length = 115\n"]},{"output_type":"stream","name":"stderr","text":["  4%|▍         | 16/360 [01:19<32:54,  5.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 109.59352517985647  episode len = 230\n","\n","Average Reward =  109.59352517985647 Average Ep_len =  230.0 \n","\n","Settings: lr = 0.0002596643209872764 epsilon = 0.7335985438522895\n","Test Result: reward = 109.59352517985647 episode length = 230\n"]},{"output_type":"stream","name":"stderr","text":["  5%|▌         | 19/360 [01:38<36:15,  6.38s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 180.5170648464175  episode len = 340\n","\n","Average Reward =  180.5170648464175 Average Ep_len =  340.0 \n","\n","Settings: lr = 0.0002522728332703459 epsilon = 0.7209948379388026\n","Test Result: reward = 180.5170648464175 episode length = 340\n"]},{"output_type":"stream","name":"stderr","text":["  6%|▌         | 22/360 [01:55<34:52,  6.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 123.67593984962474  episode len = 265\n","\n","Average Reward =  123.67593984962474 Average Ep_len =  265.0 \n","\n","Settings: lr = 0.0002450917483167285 epsilon = 0.7086076719899665\n","Test Result: reward = 123.67593984962474 episode length = 265\n"]},{"output_type":"stream","name":"stderr","text":["  7%|▋         | 25/360 [02:18<45:16,  8.11s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 541.7079136690567  episode len = 730\n","\n","Average Reward =  541.7079136690567 Average Ep_len =  730.0 \n","\n","Settings: lr = 0.00023811507689604114 epsilon = 0.6964333257066396\n","Test Result: reward = 541.7079136690567 episode length = 730\n"]},{"output_type":"stream","name":"stderr","text":["  8%|▊         | 28/360 [02:42<48:10,  8.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 341.7481481481474  episode len = 560\n","\n","Average Reward =  341.7481481481474 Average Ep_len =  560.0 \n","\n","Settings: lr = 0.00023133700026463792 epsilon = 0.6844681427068687\n","Test Result: reward = 341.7481481481474 episode length = 560\n"]},{"output_type":"stream","name":"stderr","text":["  9%|▊         | 31/360 [03:17<1:08:10, 12.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 878.5328621907947  episode len = 1140\n","\n","Average Reward =  878.5328621907947 Average Ep_len =  1140.0 \n","\n","Settings: lr = 0.00022475186531261112 epsilon = 0.6727085294277494\n","Test Result: reward = 878.5328621907947 episode length = 1140\n"]},{"output_type":"stream","name":"stderr","text":["  9%|▉         | 34/360 [03:45<1:02:52, 11.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 571.1999999999912  episode len = 910\n","\n","Average Reward =  571.1999999999912 Average Ep_len =  910.0 \n","\n","Settings: lr = 0.0002183541798489359 epsilon = 0.6611509540461538\n","Test Result: reward = 571.1999999999912 episode length = 910\n"]},{"output_type":"stream","name":"stderr","text":[" 10%|█         | 37/360 [04:17<1:06:19, 12.32s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 866.9818181818008  episode len = 1110\n","\n","Average Reward =  866.9818181818008 Average Ep_len =  1110.0 \n","\n","Settings: lr = 0.00021213860802082586 epsilon = 0.6497919454180002\n","Test Result: reward = 866.9818181818008 episode length = 1110\n"]},{"output_type":"stream","name":"stderr","text":[" 11%|█         | 40/360 [04:45<1:01:25, 11.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 525.8511705685553  episode len = 790\n","\n","Average Reward =  525.8511705685553 Average Ep_len =  790.0 \n","\n","Settings: lr = 0.00020609996586347878 epsilon = 0.6386280920357476\n","Test Result: reward = 525.8511705685553 episode length = 790\n"]},{"output_type":"stream","name":"stderr","text":[" 12%|█▏        | 43/360 [05:08<50:56,  9.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 259.93039513677945  episode len = 500\n","\n","Average Reward =  259.93039513677945 Average Ep_len =  500.0 \n","\n","Settings: lr = 0.00020023321697650197 epsilon = 0.6276560410037999\n","Test Result: reward = 259.93039513677945 episode length = 500\n"]},{"output_type":"stream","name":"stderr","text":[" 13%|█▎        | 46/360 [05:36<56:36, 10.82s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 664.6731629392862  episode len = 1020\n","\n","Average Reward =  664.6731629392862 Average Ep_len =  1020.0 \n","\n","Settings: lr = 0.0001945334683234099 epsilon = 0.6168724970315149\n","Test Result: reward = 664.6731629392862 episode length = 1020\n"]},{"output_type":"stream","name":"stderr","text":[" 14%|█▎        | 49/360 [05:59<48:39,  9.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 329.93262411347433  episode len = 490\n","\n","Average Reward =  329.93262411347433 Average Ep_len =  490.0 \n","\n","Settings: lr = 0.00018899596615069196 epsilon = 0.606274221443513\n","Test Result: reward = 329.93262411347433 episode length = 490\n"]},{"output_type":"stream","name":"stderr","text":[" 14%|█▍        | 52/360 [06:29<57:54, 11.28s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 817.6283018867798  episode len = 1065\n","\n","Average Reward =  817.6283018867798 Average Ep_len =  1065.0 \n","\n","Settings: lr = 0.0001836160920230458 epsilon = 0.5958580312069891\n","Test Result: reward = 817.6283018867798 episode length = 1065\n"]},{"output_type":"stream","name":"stderr","text":[" 15%|█▌        | 55/360 [06:54<51:02, 10.04s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 411.9100671140908  episode len = 645\n","\n","Average Reward =  411.9100671140908 Average Ep_len =  645.0 \n","\n","Settings: lr = 0.00017838935897147027 epsilon = 0.5856207979757379\n","Test Result: reward = 411.9100671140908 episode length = 645\n"]},{"output_type":"stream","name":"stderr","text":[" 16%|█▌        | 58/360 [07:13<40:37,  8.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 149.7303030303036  episode len = 280\n","\n","Average Reward =  149.7303030303036 Average Ep_len =  280.0 \n","\n","Settings: lr = 0.00017331140775100459 epsilon = 0.5755594471506006\n","Test Result: reward = 149.7303030303036 episode length = 280\n"]},{"output_type":"stream","name":"stderr","text":[" 17%|█▋        | 61/360 [07:36<41:36,  8.35s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 287.90617283950667  episode len = 515\n","\n","Average Reward =  287.90617283950667 Average Ep_len =  515.0 \n","\n","Settings: lr = 0.00016837800320499362 epsilon = 0.5656709569560564\n","Test Result: reward = 287.90617283950667 episode length = 515\n"]},{"output_type":"stream","name":"stderr","text":[" 18%|█▊        | 64/360 [08:06<52:41, 10.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 749.0315789473523  episode len = 1035\n","\n","Average Reward =  749.0315789473523 Average Ep_len =  1035.0 \n","\n","Settings: lr = 0.00016358503073284568 epsilon = 0.5559523575326769\n","Test Result: reward = 749.0315789473523 episode length = 1035\n"]},{"output_type":"stream","name":"stderr","text":[" 19%|█▊        | 67/360 [08:33<50:58, 10.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 547.6733788395808  episode len = 765\n","\n","Average Reward =  547.6733788395808 Average Ep_len =  765.0 \n","\n","Settings: lr = 0.0001589284928583382 epsilon = 0.5464007300451741\n","Test Result: reward = 547.6733788395808 episode length = 765\n"]},{"output_type":"stream","name":"stderr","text":[" 19%|█▉        | 70/360 [09:06<59:39, 12.34s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 889.4999999999827  episode len = 1100\n","\n","Average Reward =  889.4999999999827 Average Ep_len =  1100.0 \n","\n","Settings: lr = 0.0001544045058956078 epsilon = 0.5370132058057714\n","Test Result: reward = 889.4999999999827 episode length = 1100\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|██        | 73/360 [09:38<1:01:10, 12.79s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 885.3999999999819  episode len = 1145\n","\n","Average Reward =  885.3999999999819 Average Ep_len =  1145.0 \n","\n","Settings: lr = 0.00015000929671004533 epsilon = 0.5277869654126367\n","Test Result: reward = 885.3999999999819 episode length = 1145\n"]},{"output_type":"stream","name":"stderr","text":[" 21%|██        | 76/360 [10:05<54:20, 11.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 490.1118644067733  episode len = 725\n","\n","Average Reward =  490.1118644067733 Average Ep_len =  725.0 \n","\n","Settings: lr = 0.00014573919957139373 epsilon = 0.5187192379031176\n","Test Result: reward = 490.1118644067733 episode length = 725\n"]},{"output_type":"stream","name":"stderr","text":[" 22%|██▏       | 79/360 [10:36<54:56, 11.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 549.4042071197321  episode len = 845\n","\n","Average Reward =  549.4042071197321 Average Ep_len =  845.0 \n","\n","Settings: lr = 0.00014159065309642376 epsilon = 0.5098072999215241\n","Test Result: reward = 549.4042071197321 episode length = 845\n"]},{"output_type":"stream","name":"stderr","text":[" 23%|██▎       | 82/360 [10:59<45:23,  9.80s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 337.748829431437  episode len = 565\n","\n","Average Reward =  337.748829431437 Average Ep_len =  565.0 \n","\n","Settings: lr = 0.00013756019727863866 epsilon = 0.5010484749012095\n","Test Result: reward = 337.748829431437 episode length = 565\n"]},{"output_type":"stream","name":"stderr","text":[" 24%|██▎       | 85/360 [11:27<49:58, 10.90s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 835.7312977099123  episode len = 1030\n","\n","Average Reward =  835.7312977099123 Average Ep_len =  1030.0 \n","\n","Settings: lr = 0.00013364447060252972 epsilon = 0.49244013226070465\n","Test Result: reward = 835.7312977099123 episode length = 1030\n"]},{"output_type":"stream","name":"stderr","text":[" 24%|██▍       | 88/360 [12:00<56:18, 12.42s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 891.7999999999902  episode len = 1080\n","\n","Average Reward =  891.7999999999902 Average Ep_len =  1080.0 \n","\n","Settings: lr = 0.00012984020723997604 epsilon = 0.48397968661366114\n","Test Result: reward = 891.7999999999902 episode length = 1080\n"]},{"output_type":"stream","name":"stderr","text":[" 25%|██▌       | 91/360 [12:28<49:14, 10.98s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 279.25652173913005  episode len = 575\n","\n","Average Reward =  279.25652173913005 Average Ep_len =  575.0 \n","\n","Settings: lr = 0.00012614423432645043 epsilon = 0.4756645969923705\n","Test Result: reward = 279.25652173913005 episode length = 575\n"]},{"output_type":"stream","name":"stderr","text":[" 26%|██▌       | 94/360 [12:52<44:22, 10.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 291.0061855670103  episode len = 490\n","\n","Average Reward =  291.0061855670103 Average Ep_len =  490.0 \n","\n","Settings: lr = 0.00012255346931475963 epsilon = 0.46749236608462175\n","Test Result: reward = 291.0061855670103 episode length = 490\n"]},{"output_type":"stream","name":"stderr","text":[" 27%|██▋       | 97/360 [13:15<40:08,  9.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 363.4725631768924  episode len = 550\n","\n","Average Reward =  363.4725631768924 Average Ep_len =  550.0 \n","\n","Settings: lr = 0.00011906491740411169 epsilon = 0.45946053948367216\n","Test Result: reward = 363.4725631768924 episode length = 550\n"]},{"output_type":"stream","name":"stderr","text":[" 28%|██▊       | 100/360 [13:46<48:44, 11.25s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 656.2221476509955  episode len = 920\n","\n","Average Reward =  656.2221476509955 Average Ep_len =  920.0 \n","\n","Settings: lr = 0.00011567566904236637 epsilon = 0.4515667049511022\n","Test Result: reward = 656.2221476509955 episode length = 920\n"]},{"output_type":"stream","name":"stderr","text":[" 29%|██▊       | 103/360 [14:07<37:21,  8.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 193.87142857142987  episode len = 345\n","\n","Average Reward =  193.87142857142987 Average Ep_len =  345.0 \n","\n","Settings: lr = 0.00011238289749938545 epsilon = 0.4438084916923365\n","Test Result: reward = 193.87142857142987 episode length = 345\n"]},{"output_type":"stream","name":"stderr","text":[" 29%|██▉       | 106/360 [14:40<49:34, 11.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 865.1385964912097  episode len = 1100\n","\n","Average Reward =  865.1385964912097 Average Ep_len =  1100.0 \n","\n","Settings: lr = 0.0001091838565094588 epsilon = 0.43618356964461136\n","Test Result: reward = 865.1385964912097 episode length = 1100\n"]},{"output_type":"stream","name":"stderr","text":[" 30%|███       | 109/360 [15:17<57:58, 13.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 854.9919614147725  episode len = 1220\n","\n","Average Reward =  854.9919614147725 Average Ep_len =  1220.0 \n","\n","Settings: lr = 0.00010607587798084027 epsilon = 0.42868964877717497\n","Test Result: reward = 854.9919614147725 episode length = 1220\n"]},{"output_type":"stream","name":"stderr","text":[" 31%|███       | 112/360 [15:36<38:11,  9.24s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 130.21584158415905  episode len = 280\n","\n","Average Reward =  130.21584158415905 Average Ep_len =  280.0 \n","\n","Settings: lr = 0.0001030563697704827 epsilon = 0.42132447840351156\n","Test Result: reward = 130.21584158415905 episode length = 280\n"]},{"output_type":"stream","name":"stderr","text":[" 32%|███▏      | 115/360 [16:11<49:27, 12.11s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 885.4999999999801  episode len = 1140\n","\n","Average Reward =  885.4999999999801 Average Ep_len =  1140.0 \n","\n","Settings: lr = 0.00010012281352211659 epsilon = 0.4140858465053812\n","Test Result: reward = 885.4999999999801 episode length = 1140\n"]},{"output_type":"stream","name":"stderr","text":[" 33%|███▎      | 118/360 [16:45<51:42, 12.82s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 735.8838283828281  episode len = 1020\n","\n","Average Reward =  735.8838283828281 Average Ep_len =  1020.0 \n","\n","Settings: lr = 9.72727625658687e-05 epsilon = 0.4069715790684737\n","Test Result: reward = 735.8838283828281 episode length = 1020\n"]},{"output_type":"stream","name":"stderr","text":[" 34%|███▎      | 121/360 [17:07<38:33,  9.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 218.92899022801447  episode len = 380\n","\n","Average Reward =  218.92899022801447 Average Ep_len =  380.0 \n","\n","Settings: lr = 9.450383987766947e-05 epsilon = 0.39997953942947573\n","Test Result: reward = 218.92899022801447 episode length = 380\n"]},{"output_type":"stream","name":"stderr","text":[" 34%|███▍      | 124/360 [17:41<47:23, 12.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 883.6999999999787  episode len = 1160\n","\n","Average Reward =  883.6999999999787 Average Ep_len =  1160.0 \n","\n","Settings: lr = 9.181373609674692e-05 epsilon = 0.3931076276343562\n","Test Result: reward = 883.6999999999787 episode length = 1160\n"]},{"output_type":"stream","name":"stderr","text":[" 35%|███▌      | 127/360 [18:18<53:26, 13.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 874.9142857142679  episode len = 1105\n","\n","Average Reward =  874.9142857142679 Average Ep_len =  1105.0 \n","\n","Settings: lr = 8.92002075995536e-05 epsilon = 0.3863537798076768\n","Test Result: reward = 874.9142857142679 episode length = 1105\n"]},{"output_type":"stream","name":"stderr","text":[" 36%|███▌      | 130/360 [18:55<54:46, 14.29s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 887.7999999999843  episode len = 1120\n","\n","Average Reward =  887.7999999999843 Average Ep_len =  1120.0 \n","\n","Settings: lr = 8.666107462852034e-05 epsilon = 0.37971596753273795\n","Test Result: reward = 887.7999999999843 episode length = 1120\n"]},{"output_type":"stream","name":"stderr","text":[" 37%|███▋      | 133/360 [19:29<51:06, 13.51s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 885.3664310953891  episode len = 1110\n","\n","Average Reward =  885.3664310953891 Average Ep_len =  1110.0 \n","\n","Settings: lr = 8.419421947407619e-05 epsilon = 0.373192197242374\n","Test Result: reward = 885.3664310953891 episode length = 1110\n"]},{"output_type":"stream","name":"stderr","text":[" 38%|███▊      | 136/360 [19:59<43:07, 11.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 352.8591240875895  episode len = 555\n","\n","Average Reward =  352.8591240875895 Average Ep_len =  555.0 \n","\n","Settings: lr = 8.179758470841781e-05 epsilon = 0.36678050962021597\n","Test Result: reward = 352.8591240875895 episode length = 555\n"]},{"output_type":"stream","name":"stderr","text":[" 39%|███▊      | 139/360 [20:30<44:11, 12.00s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 884.2734693877386  episode len = 990\n","\n","Average Reward =  884.2734693877386 Average Ep_len =  990.0 \n","\n","Settings: lr = 7.946917146955593e-05 epsilon = 0.3604789790122397\n","Test Result: reward = 884.2734693877386 episode length = 990\n"]},{"output_type":"stream","name":"stderr","text":[" 39%|███▉      | 142/360 [20:53<33:53,  9.33s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 184.21428571428703  episode len = 385\n","\n","Average Reward =  184.21428571428703 Average Ep_len =  385.0 \n","\n","Settings: lr = 7.720703779420727e-05 epsilon = 0.3542857128484247\n","Test Result: reward = 184.21428571428703 episode length = 385\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|████      | 145/360 [21:29<44:43, 12.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 866.6814432989497  episode len = 1125\n","\n","Average Reward =  866.6814432989497 Average Ep_len =  1125.0 \n","\n","Settings: lr = 7.500929699814144e-05 epsilon = 0.34819885107434967\n","Test Result: reward = 866.6814432989497 episode length = 1125\n"]},{"output_type":"stream","name":"stderr","text":[" 41%|████      | 148/360 [21:54<37:46, 10.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 377.06370106761347  episode len = 570\n","\n","Average Reward =  377.06370106761347 Average Ep_len =  570.0 \n","\n","Settings: lr = 7.28741161026324e-05 epsilon = 0.3422165655925525\n","Test Result: reward = 377.06370106761347 episode length = 570\n"]},{"output_type":"stream","name":"stderr","text":[" 42%|████▏     | 151/360 [22:20<34:53, 10.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 241.6117647058839  episode len = 375\n","\n","Average Reward =  241.6117647058839 Average Ep_len =  375.0 \n","\n","Settings: lr = 7.079971430570178e-05 epsilon = 0.3363370597134889\n","Test Result: reward = 241.6117647058839 episode length = 375\n"]},{"output_type":"stream","name":"stderr","text":[" 43%|████▎     | 154/360 [22:47<33:16,  9.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 199.77500000000106  episode len = 345\n","\n","Average Reward =  199.77500000000106 Average Ep_len =  345.0 \n","\n","Settings: lr = 6.878436149687895e-05 epsilon = 0.3305585676159238\n","Test Result: reward = 199.77500000000106 episode length = 345\n"]},{"output_type":"stream","name":"stderr","text":[" 44%|████▎     | 157/360 [23:10<27:45,  8.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 60.07589576547214  episode len = 180\n","\n","Average Reward =  60.07589576547214 Average Ep_len =  180.0 \n","\n","Settings: lr = 6.68263768142394e-05 epsilon = 0.32487935381659344\n","Test Result: reward = 60.07589576547214 episode length = 180\n"]},{"output_type":"stream","name":"stderr","text":[" 44%|████▍     | 160/360 [23:39<34:05, 10.23s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 546.4863481228571  episode len = 810\n","\n","Average Reward =  546.4863481228571 Average Ep_len =  810.0 \n","\n","Settings: lr = 6.492412724251782e-05 epsilon = 0.3192977126489789\n","Test Result: reward = 546.4863481228571 episode length = 810\n"]},{"output_type":"stream","name":"stderr","text":[" 45%|████▌     | 163/360 [24:01<28:52,  8.79s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 273.38953068592065  episode len = 440\n","\n","Average Reward =  273.38953068592065 Average Ep_len =  440.0 \n","\n","Settings: lr = 6.307602625112664e-05 epsilon = 0.313811967751035\n","Test Result: reward = 273.38953068592065 episode length = 440\n"]},{"output_type":"stream","name":"stderr","text":[" 46%|████▌     | 166/360 [24:23<27:45,  8.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 268.29090909090917  episode len = 405\n","\n","Average Reward =  268.29090909090917 Average Ep_len =  405.0 \n","\n","Settings: lr = 6.128053247094406e-05 epsilon = 0.30842047156172\n","Test Result: reward = 268.29090909090917 episode length = 405\n"]},{"output_type":"stream","name":"stderr","text":[" 47%|████▋     | 169/360 [24:54<35:18, 11.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 864.7486590038212  episode len = 1045\n","\n","Average Reward =  864.7486590038212 Average Ep_len =  1045.0 \n","\n","Settings: lr = 5.953614840876811e-05 epsilon = 0.30312160482617534\n","Test Result: reward = 864.7486590038212 episode length = 1045\n"]},{"output_type":"stream","name":"stderr","text":[" 48%|████▊     | 172/360 [25:28<39:01, 12.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 826.5215946843729  episode len = 1165\n","\n","Average Reward =  826.5215946843729 Average Ep_len =  1165.0 \n","\n","Settings: lr = 5.784141919836449e-05 epsilon = 0.2979137761094071\n","Test Result: reward = 826.5215946843729 episode length = 1165\n"]},{"output_type":"stream","name":"stderr","text":[" 49%|████▊     | 175/360 [25:53<32:08, 10.42s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 340.0174496644283  episode len = 525\n","\n","Average Reward =  340.0174496644283 Average Ep_len =  525.0 \n","\n","Settings: lr = 5.619493138706643e-05 epsilon = 0.2927954213183221\n","Test Result: reward = 340.0174496644283 episode length = 525\n"]},{"output_type":"stream","name":"stderr","text":[" 49%|████▉     | 178/360 [26:14<26:12,  8.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 276.18469750889744  episode len = 440\n","\n","Average Reward =  276.18469750889744 Average Ep_len =  440.0 \n","\n","Settings: lr = 5.4595311756914746e-05 epsilon = 0.2877650032319761\n","Test Result: reward = 276.18469750889744 episode length = 440\n"]},{"output_type":"stream","name":"stderr","text":[" 50%|█████     | 181/360 [26:36<24:31,  8.22s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 192.6581699346417  episode len = 360\n","\n","Average Reward =  192.6581699346417 Average Ep_len =  360.0 \n","\n","Settings: lr = 5.304122617935478e-05 epsilon = 0.28282101103989277\n","Test Result: reward = 192.6581699346417 episode length = 360\n"]},{"output_type":"stream","name":"stderr","text":[" 51%|█████     | 184/360 [26:58<23:58,  8.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 270.6014084507052  episode len = 460\n","\n","Average Reward =  270.6014084507052 Average Ep_len =  460.0 \n","\n","Settings: lr = 5.153137850253497e-05 epsilon = 0.2779619598883142\n","Test Result: reward = 270.6014084507052 episode length = 460\n"]},{"output_type":"stream","name":"stderr","text":[" 52%|█████▏    | 187/360 [27:24<26:47,  9.29s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 314.24966887417145  episode len = 530\n","\n","Average Reward =  314.24966887417145 Average Ep_len =  530.0 \n","\n","Settings: lr = 5.00645094702791e-05 epsilon = 0.27318639043424753\n","Test Result: reward = 314.24966887417145 episode length = 530\n"]},{"output_type":"stream","name":"stderr","text":[" 53%|█████▎    | 190/360 [27:47<24:09,  8.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 163.07586206896647  episode len = 350\n","\n","Average Reward =  163.07586206896647 Average Ep_len =  350.0 \n","\n","Settings: lr = 4.863939567183063e-05 epsilon = 0.26849286840717335\n","Test Result: reward = 163.07586206896647 episode length = 350\n"]},{"output_type":"stream","name":"stderr","text":[" 54%|█████▎    | 193/360 [28:03<18:47,  6.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 45.340590405903946  episode len = 135\n","\n","Average Reward =  45.340590405903946 Average Ep_len =  135.0 \n","\n","Settings: lr = 4.7254848521493125e-05 epsilon = 0.2638799841782838\n","Test Result: reward = 45.340590405903946 episode length = 135\n"]},{"output_type":"stream","name":"stderr","text":[" 54%|█████▍    | 196/360 [28:20<16:39,  6.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 43.037588652482164  episode len = 135\n","\n","Average Reward =  43.037588652482164 Average Ep_len =  135.0 \n","\n","Settings: lr = 4.5909713267315705e-05 epsilon = 0.259346352337122\n","Test Result: reward = 43.037588652482164 episode length = 135\n"]},{"output_type":"stream","name":"stderr","text":[" 55%|█████▌    | 199/360 [28:47<22:43,  8.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 309.66946107784383  episode len = 555\n","\n","Average Reward =  309.66946107784383 Average Ep_len =  555.0 \n","\n","Settings: lr = 4.4602868027996924e-05 epsilon = 0.25489061127549467\n","Test Result: reward = 309.66946107784383 episode length = 555\n"]},{"output_type":"stream","name":"stderr","text":[" 56%|█████▌    | 202/360 [29:27<33:44, 12.81s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 866.5292418772465  episode len = 1080\n","\n","Average Reward =  866.5292418772465 Average Ep_len =  1080.0 \n","\n","Settings: lr = 4.3333222857203645e-05 epsilon = 0.2505114227785338\n","Test Result: reward = 866.5292418772465 episode length = 1080\n"]},{"output_type":"stream","name":"stderr","text":[" 57%|█████▋    | 205/360 [29:57<31:29, 12.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 761.1061302681903  episode len = 930\n","\n","Average Reward =  761.1061302681903 Average Ep_len =  930.0 \n","\n","Settings: lr = 4.209971883452459e-05 epsilon = 0.24620747162278345\n","Test Result: reward = 761.1061302681903 episode length = 930\n"]},{"output_type":"stream","name":"stderr","text":[" 58%|█████▊    | 208/360 [30:24<26:46, 10.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 224.42352941176537  episode len = 340\n","\n","Average Reward =  224.42352941176537 Average Ep_len =  340.0 \n","\n","Settings: lr = 4.090132718230039e-05 epsilon = 0.24197746518119276\n","Test Result: reward = 224.42352941176537 episode length = 340\n"]},{"output_type":"stream","name":"stderr","text":[" 59%|█████▊    | 211/360 [30:51<25:58, 10.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 499.5747603833802  episode len = 785\n","\n","Average Reward =  499.5747603833802 Average Ep_len =  785.0 \n","\n","Settings: lr = 3.973704840759363e-05 epsilon = 0.2378201330348944\n","Test Result: reward = 499.5747603833802 episode length = 785\n"]},{"output_type":"stream","name":"stderr","text":[" 59%|█████▉    | 214/360 [31:14<22:05,  9.08s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 170.25479452054867  episode len = 315\n","\n","Average Reward =  170.25479452054867 Average Ep_len =  315.0 \n","\n","Settings: lr = 3.8605911468583084e-05 epsilon = 0.2337342265916536\n","Test Result: reward = 170.25479452054867 episode length = 315\n"]},{"output_type":"stream","name":"stderr","text":[" 60%|██████    | 217/360 [31:35<19:08,  8.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 163.14628975265083  episode len = 310\n","\n","Average Reward =  163.14628975265083 Average Ep_len =  310.0 \n","\n","Settings: lr = 3.7506972964687045e-05 epsilon = 0.22971851871087204\n","Test Result: reward = 163.14628975265083 episode length = 310\n"]},{"output_type":"stream","name":"stderr","text":[" 61%|██████    | 220/360 [31:59<19:37,  8.41s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 205.2181184668999  episode len = 350\n","\n","Average Reward =  205.2181184668999 Average Ep_len =  350.0 \n","\n","Settings: lr = 3.643931634974026e-05 epsilon = 0.22577180333503477\n","Test Result: reward = 205.2181184668999 episode length = 350\n"]},{"output_type":"stream","name":"stderr","text":[" 62%|██████▏   | 223/360 [32:22<19:03,  8.35s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 217.99706744868195  episode len = 425\n","\n","Average Reward =  217.99706744868195 Average Ep_len =  425.0 \n","\n","Settings: lr = 3.540205116756821e-05 epsilon = 0.22189289512748883\n","Test Result: reward = 217.99706744868195 episode length = 425\n"]},{"output_type":"stream","name":"stderr","text":[" 63%|██████▎   | 226/360 [32:42<16:52,  7.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 141.91379310344908  episode len = 300\n","\n","Average Reward =  141.91379310344908 Average Ep_len =  300.0 \n","\n","Settings: lr = 3.4394312309321114e-05 epsilon = 0.21808062911644535\n","Test Result: reward = 141.91379310344908 episode length = 300\n"]},{"output_type":"stream","name":"stderr","text":[" 64%|██████▎   | 229/360 [33:01<14:49,  6.79s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 53.444444444444315  episode len = 155\n","\n","Average Reward =  53.444444444444315 Average Ep_len =  155.0 \n","\n","Settings: lr = 3.3415259291948445e-05 epsilon = 0.2143338603450977\n","Test Result: reward = 53.444444444444315 episode length = 155\n"]},{"output_type":"stream","name":"stderr","text":[" 64%|██████▍   | 232/360 [33:17<12:59,  6.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 97.77394636015346  episode len = 205\n","\n","Average Reward =  97.77394636015346 Average Ep_len =  205.0 \n","\n","Settings: lr = 3.246407555721199e-05 epsilon = 0.21065146352775077\n","Test Result: reward = 97.77394636015346 episode length = 205\n"]},{"output_type":"stream","name":"stderr","text":[" 65%|██████▌   | 235/360 [33:32<11:43,  5.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 56.42727272727255  episode len = 160\n","\n","Average Reward =  56.42727272727255 Average Ep_len =  160.0 \n","\n","Settings: lr = 3.153996779065289e-05 epsilon = 0.20703233271185872\n","Test Result: reward = 56.42727272727255 episode length = 160\n"]},{"output_type":"stream","name":"stderr","text":[" 66%|██████▌   | 238/360 [33:47<10:55,  5.37s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 59.271698113207414  episode len = 160\n","\n","Average Reward =  59.271698113207414 Average Ep_len =  160.0 \n","\n","Settings: lr = 3.064216525994472e-05 epsilon = 0.20347538094586823\n","Test Result: reward = 59.271698113207414 episode length = 160\n"]},{"output_type":"stream","name":"stderr","text":[" 67%|██████▋   | 241/360 [34:04<11:58,  6.04s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 67.00858085808581  episode len = 185\n","\n","Average Reward =  67.00858085808581 Average Ep_len =  185.0 \n","\n","Settings: lr = 2.976991917208064e-05 epsilon = 0.1999795399527694\n","Test Result: reward = 67.00858085808581 episode length = 185\n"]},{"output_type":"stream","name":"stderr","text":[" 68%|██████▊   | 244/360 [34:23<12:50,  6.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 261.0000000000002  episode len = 385\n","\n","Average Reward =  261.0000000000002 Average Ep_len =  385.0 \n","\n","Settings: lr = 2.892250204885858e-05 epsilon = 0.19654375980925454\n","Test Result: reward = 261.0000000000002 episode length = 385\n"]},{"output_type":"stream","name":"stderr","text":[" 69%|██████▊   | 247/360 [34:38<10:44,  5.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 56.00216606498182  episode len = 160\n","\n","Average Reward =  56.00216606498182 Average Ep_len =  160.0 \n","\n","Settings: lr = 2.809920712014363e-05 epsilon = 0.19316700863038955\n","Test Result: reward = 56.00216606498182 episode length = 160\n"]},{"output_type":"stream","name":"stderr","text":[" 69%|██████▉   | 250/360 [34:56<11:17,  6.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 57.73309608540911  episode len = 165\n","\n","Average Reward =  57.73309608540911 Average Ep_len =  165.0 \n","\n","Settings: lr = 2.729934773440151e-05 epsilon = 0.18984827225970283\n","Test Result: reward = 57.73309608540911 episode length = 165\n"]},{"output_type":"stream","name":"stderr","text":[" 70%|███████   | 253/360 [35:14<11:08,  6.25s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 81.97519379844977  episode len = 185\n","\n","Average Reward =  81.97519379844977 Average Ep_len =  185.0 \n","\n","Settings: lr = 2.652225678601153e-05 epsilon = 0.18658655396459853\n","Test Result: reward = 81.97519379844977 episode length = 185\n"]},{"output_type":"stream","name":"stderr","text":[" 71%|███████   | 256/360 [35:30<10:10,  5.87s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 52.74126984126972  episode len = 170\n","\n","Average Reward =  52.74126984126972 Average Ep_len =  170.0 \n","\n","Settings: lr = 2.5767286158881407e-05 epsilon = 0.18338087413700296\n","Test Result: reward = 52.74126984126972 episode length = 170\n"]},{"output_type":"stream","name":"stderr","text":[" 72%|███████▏  | 259/360 [35:47<10:27,  6.22s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 46.299999999999876  episode len = 160\n","\n","Average Reward =  46.299999999999876 Average Ep_len =  160.0 \n","\n","Settings: lr = 2.5033806185899917e-05 epsilon = 0.18023026999915406\n","Test Result: reward = 46.299999999999876 episode length = 160\n"]},{"output_type":"stream","name":"stderr","text":[" 73%|███████▎  | 262/360 [36:04<09:37,  5.90s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 51.537809187278995  episode len = 155\n","\n","Average Reward =  51.537809187278995 Average Ep_len =  155.0 \n","\n","Settings: lr = 2.4321205123776464e-05 epsilon = 0.1771337953144455\n","Test Result: reward = 51.537809187278995 episode length = 155\n"]},{"output_type":"stream","name":"stderr","text":[" 74%|███████▎  | 265/360 [36:21<09:17,  5.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 49.806600660065854  episode len = 160\n","\n","Average Reward =  49.806600660065854 Average Ep_len =  160.0 \n","\n","Settings: lr = 2.3628888642829706e-05 epsilon = 0.17409052010323875\n","Test Result: reward = 49.806600660065854 episode length = 160\n"]},{"output_type":"stream","name":"stderr","text":[" 74%|███████▍  | 268/360 [36:35<08:14,  5.38s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 53.386411149825626  episode len = 160\n","\n","Average Reward =  53.386411149825626 Average Ep_len =  160.0 \n","\n","Settings: lr = 2.295627933129956e-05 epsilon = 0.17109953036355763\n","Test Result: reward = 53.386411149825626 episode length = 160\n"]},{"output_type":"stream","name":"stderr","text":[" 75%|███████▌  | 271/360 [36:51<08:01,  5.41s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 45.98831168831155  episode len = 155\n","\n","Average Reward =  45.98831168831155 Average Ep_len =  155.0 \n","\n","Settings: lr = 2.2302816213769293e-05 epsilon = 0.16815992779658168\n","Test Result: reward = 45.98831168831155 episode length = 155\n"]},{"output_type":"stream","name":"stderr","text":[" 76%|███████▌  | 274/360 [37:06<07:43,  5.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 53.24444444444431  episode len = 160\n","\n","Average Reward =  53.24444444444431 Average Ep_len =  160.0 \n","\n","Settings: lr = 2.166795428329594e-05 epsilon = 0.1652708295368555\n","Test Result: reward = 53.24444444444431 episode length = 160\n"]},{"output_type":"stream","name":"stderr","text":[" 77%|███████▋  | 277/360 [37:23<07:58,  5.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 52.52852233676964  episode len = 160\n","\n","Average Reward =  52.52852233676964 Average Ep_len =  160.0 \n","\n","Settings: lr = 2.1051164046858948e-05 epsilon = 0.16243136788713342\n","Test Result: reward = 52.52852233676964 episode length = 160\n"]},{"output_type":"stream","name":"stderr","text":[" 78%|███████▊  | 280/360 [37:39<07:29,  5.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 51.881818181818026  episode len = 160\n","\n","Average Reward =  51.881818181818026 Average Ep_len =  160.0 \n","\n","Settings: lr = 2.045193108374781e-05 epsilon = 0.1596406900577796\n","Test Result: reward = 51.881818181818026 episode length = 160\n"]},{"output_type":"stream","name":"stderr","text":[" 79%|███████▊  | 283/360 [37:55<07:08,  5.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 54.645945945945826  episode len = 160\n","\n","Average Reward =  54.645945945945826 Average Ep_len =  160.0 \n","\n","Settings: lr = 1.9869755616520497e-05 epsilon = 0.15689795791064556\n","Test Result: reward = 54.645945945945826 episode length = 160\n"]},{"output_type":"stream","name":"stderr","text":[" 79%|███████▉  | 286/360 [38:10<06:41,  5.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 70.1321543408361  episode len = 195\n","\n","Average Reward =  70.1321543408361 Average Ep_len =  195.0 \n","\n","Settings: lr = 1.9304152094174742e-05 epsilon = 0.1542023477073481\n","Test Result: reward = 70.1321543408361 episode length = 195\n"]},{"output_type":"stream","name":"stderr","text":[" 80%|████████  | 289/360 [38:26<06:27,  5.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 29.433546325878535  episode len = 120\n","\n","Average Reward =  29.433546325878535 Average Ep_len =  120.0 \n","\n","Settings: lr = 1.875464878718463e-05 epsilon = 0.15155304986187143\n","Test Result: reward = 29.433546325878535 episode length = 120\n"]},{"output_type":"stream","name":"stderr","text":[" 81%|████████  | 292/360 [38:39<05:37,  4.96s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 25.80087463556845  episode len = 120\n","\n","Average Reward =  25.80087463556845 Average Ep_len =  120.0 \n","\n","Settings: lr = 1.822078739406465e-05 epsilon = 0.14894926869742073\n","Test Result: reward = 25.80087463556845 episode length = 120\n"]},{"output_type":"stream","name":"stderr","text":[" 82%|████████▏ | 295/360 [38:53<05:11,  4.79s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 34.49498207885296  episode len = 120\n","\n","Average Reward =  34.49498207885296 Average Ep_len =  120.0 \n","\n","Settings: lr = 1.770212265913316e-05 epsilon = 0.1463902222074522\n","Test Result: reward = 34.49498207885296 episode length = 120\n"]},{"output_type":"stream","name":"stderr","text":[" 83%|████████▎ | 298/360 [39:10<05:52,  5.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 72.56770186335412  episode len = 205\n","\n","Average Reward =  72.56770186335412 Average Ep_len =  205.0 \n","\n","Settings: lr = 1.719822200115639e-05 epsilon = 0.14387514182080924\n","Test Result: reward = 72.56770186335412 episode length = 205\n"]},{"output_type":"stream","name":"stderr","text":[" 84%|████████▎ | 301/360 [39:31<06:38,  6.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 155.25338078291895  episode len = 295\n","\n","Average Reward =  155.25338078291895 Average Ep_len =  295.0 \n","\n","Settings: lr = 1.6708665152563318e-05 epsilon = 0.1414032721708937\n","Test Result: reward = 155.25338078291895 episode length = 295\n"]},{"output_type":"stream","name":"stderr","text":[" 84%|████████▍ | 304/360 [39:50<06:25,  6.89s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 110.83333333333371  episode len = 220\n","\n","Average Reward =  110.83333333333371 Average Ep_len =  220.0 \n","\n","Settings: lr = 1.623304380893048e-05 epsilon = 0.13897387086880286\n","Test Result: reward = 110.83333333333371 episode length = 220\n"]},{"output_type":"stream","name":"stderr","text":[" 85%|████████▌ | 307/360 [40:09<05:57,  6.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 106.37756653992437  episode len = 225\n","\n","Average Reward =  106.37756653992437 Average Ep_len =  225.0 \n","\n","Settings: lr = 1.577096128844441e-05 epsilon = 0.13658620828036405\n","Test Result: reward = 106.37756653992437 episode length = 225\n"]},{"output_type":"stream","name":"stderr","text":[" 86%|████████▌ | 310/360 [40:28<05:36,  6.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 83.00622837370257  episode len = 205\n","\n","Average Reward =  83.00622837370257 Average Ep_len =  205.0 \n","\n","Settings: lr = 1.5322032201057643e-05 epsilon = 0.134239567307\n","Test Result: reward = 83.00622837370257 episode length = 205\n"]},{"output_type":"stream","name":"stderr","text":[" 87%|████████▋ | 313/360 [40:49<05:43,  7.32s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 175.9219931271485  episode len = 335\n","\n","Average Reward =  175.9219931271485 Average Ep_len =  335.0 \n","\n","Settings: lr = 1.4885882127062377e-05 epsilon = 0.13193324317035912\n","Test Result: reward = 175.9219931271485 episode length = 335\n"]},{"output_type":"stream","name":"stderr","text":[" 88%|████████▊ | 316/360 [41:09<05:26,  7.42s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 169.1042253521136  episode len = 315\n","\n","Average Reward =  169.1042253521136 Average Ep_len =  315.0 \n","\n","Settings: lr = 1.4462147304813739e-05 epsilon = 0.12966654320064572\n","Test Result: reward = 169.1042253521136 episode length = 315\n"]},{"output_type":"stream","name":"stderr","text":[" 89%|████████▊ | 319/360 [41:26<04:27,  6.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 33.383850931676946  episode len = 130\n","\n","Average Reward =  33.383850931676946 Average Ep_len =  130.0 \n","\n","Settings: lr = 1.4050474327342151e-05 epsilon = 0.12743878662858737\n","Test Result: reward = 33.383850931676946 episode length = 130\n"]},{"output_type":"stream","name":"stderr","text":[" 89%|████████▉ | 322/360 [41:40<03:23,  5.36s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 36.872180451127726  episode len = 115\n","\n","Average Reward =  36.872180451127726 Average Ep_len =  115.0 \n","\n","Settings: lr = 1.3650519847601804e-05 epsilon = 0.12524930438097576\n","Test Result: reward = 36.872180451127726 episode length = 115\n"]},{"output_type":"stream","name":"stderr","text":[" 90%|█████████ | 325/360 [41:55<03:06,  5.32s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 75.82033898305087  episode len = 170\n","\n","Average Reward =  75.82033898305087 Average Ep_len =  170.0 \n","\n","Settings: lr = 1.3261950292109393e-05 epsilon = 0.12309743887972079\n","Test Result: reward = 75.82033898305087 episode length = 170\n"]},{"output_type":"stream","name":"stderr","text":[" 91%|█████████ | 328/360 [42:10<02:49,  5.29s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 37.305050505050424  episode len = 130\n","\n","Average Reward =  37.305050505050424 Average Ep_len =  130.0 \n","\n","Settings: lr = 1.2884441582734289e-05 epsilon = 0.12098254384435683\n","Test Result: reward = 37.305050505050424 episode length = 130\n"]},{"output_type":"stream","name":"stderr","text":[" 92%|█████████▏| 331/360 [42:23<02:22,  4.91s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 36.304950495049404  episode len = 130\n","\n","Average Reward =  36.304950495049404 Average Ep_len =  130.0 \n","\n","Settings: lr = 1.2517678866408098e-05 epsilon = 0.11890398409794212\n","Test Result: reward = 36.304950495049404 episode length = 130\n"]},{"output_type":"stream","name":"stderr","text":[" 93%|█████████▎| 334/360 [42:39<02:22,  5.49s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 42.17840531561451  episode len = 140\n","\n","Average Reward =  42.17840531561451 Average Ep_len =  140.0 \n","\n","Settings: lr = 1.2161356252528197e-05 epsilon = 0.11686113537629289\n","Test Result: reward = 42.17840531561451 episode length = 140\n"]},{"output_type":"stream","name":"stderr","text":[" 94%|█████████▎| 337/360 [42:53<01:54,  4.99s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 36.52280701754378  episode len = 125\n","\n","Average Reward =  36.52280701754378 Average Ep_len =  125.0 \n","\n","Settings: lr = 1.1815176557836208e-05 epsilon = 0.11485338414049498\n","Test Result: reward = 36.52280701754378 episode length = 125\n"]},{"output_type":"stream","name":"stderr","text":[" 94%|█████████▍| 340/360 [43:10<01:56,  5.81s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 164.99844961240373  episode len = 285\n","\n","Average Reward =  164.99844961240373 Average Ep_len =  285.0 \n","\n","Settings: lr = 1.147885105855866e-05 epsilon = 0.11288012739263671\n","Test Result: reward = 164.99844961240373 episode length = 285\n"]},{"output_type":"stream","name":"stderr","text":[" 95%|█████████▌| 343/360 [43:27<01:40,  5.90s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 24.89870550161807  episode len = 105\n","\n","Average Reward =  24.89870550161807 Average Ep_len =  105.0 \n","\n","Settings: lr = 1.1152099249603095e-05 epsilon = 0.11094077249470742\n","Test Result: reward = 24.89870550161807 episode length = 105\n"]},{"output_type":"stream","name":"stderr","text":[" 96%|█████████▌| 346/360 [43:48<01:38,  7.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 240.16348122867046  episode len = 395\n","\n","Average Reward =  240.16348122867046 Average Ep_len =  395.0 \n","\n","Settings: lr = 1.0834648610608797e-05 epsilon = 0.10903473699060767\n","Test Result: reward = 240.16348122867046 episode length = 395\n"]},{"output_type":"stream","name":"stderr","text":[" 97%|█████████▋| 349/360 [44:11<01:30,  8.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 331.28291814946516  episode len = 490\n","\n","Average Reward =  331.28291814946516 Average Ep_len =  490.0 \n","\n","Settings: lr = 1.0526234378657011e-05 epsilon = 0.10716144843121718\n","Test Result: reward = 331.28291814946516 episode length = 490\n"]},{"output_type":"stream","name":"stderr","text":[" 98%|█████████▊| 352/360 [44:27<00:53,  6.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 158.11118881118955  episode len = 305\n","\n","Average Reward =  158.11118881118955 Average Ep_len =  305.0 \n","\n","Settings: lr = 1.0226599327451084e-05 epsilon = 0.10532034420246845\n","Test Result: reward = 158.11118881118955 episode length = 305\n"]},{"output_type":"stream","name":"stderr","text":[" 99%|█████████▊| 355/360 [44:46<00:33,  6.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 112.48411552346606  episode len = 245\n","\n","Average Reward =  112.48411552346606 Average Ep_len =  245.0 \n","\n","Settings: lr = 9.93549355278238e-06 epsilon = 0.10351087135637403\n","Test Result: reward = 112.48411552346606 episode length = 245\n"]},{"output_type":"stream","name":"stderr","text":[" 99%|█████████▉| 358/360 [45:01<00:12,  6.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Test 1/1: reward = 142.33170731707372  episode len = 280\n","\n","Average Reward =  142.33170731707372 Average Ep_len =  280.0 \n","\n","Settings: lr = 9.65267426410301e-06 epsilon = 0.10173248644495686\n","Test Result: reward = 142.33170731707372 episode length = 280\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 360/360 [45:11<00:00,  7.53s/it]\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Go4nqY0dtXJ"},"outputs":[],"source":["env = wrap_env(gym.make(\"CarRacing-v1\").unwrapped)\n","agent = DQN_Network()\n","agent.load_state_dict(torch.load(\"car-racing-dqn.pth\"))\n","\n","simulate(agent=agent,env=env,render=True)\n","#test_model(agent,env,episodes=10)"]},{"cell_type":"code","source":["print(\"lr_hist\")\n","hist = load_list(\"lr_hist.data\")\n","for stage in hist:\n","  print(stage)\n","print(\"\\nepsilon_hist\")\n","hist = load_list(\"epsilon_hist.data\")\n","for stage in hist:\n","  print(stage)\n","print(\"\\nep_len_hist\")\n","hist = load_list(\"ep_len_hist.data\")\n","for stage in hist:\n","  print(st.mean(stage))\n","print(\"\\nreward_hist\")\n","hist = load_list(\"reward_hist.data\")\n","for stage in hist:\n","  print(st.mean(stage))"],"metadata":{"id":"JUIN0LDh0n3B","colab":{"base_uri":"https://localhost:8080/","height":617},"executionInfo":{"status":"error","timestamp":1655609140842,"user_tz":240,"elapsed":175,"user":{"displayName":"tigerfan707","userId":"17162744304693628162"}},"outputId":"3b08f322-7a3f-494e-f436-d9fbc7a53876"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["lr_hist\n","0.0005\n","0.0005\n","0.00025\n","0.00025\n","0.000125\n","0.000125\n","6.2e-05\n","6.2e-05\n","3.1e-05\n","3.1e-05\n","1.6e-05\n","1.6e-05\n","\n","epsilon_hist\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-dd7e196c0e70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nepsilon_hist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epsilon_hist.data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mload_list\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfilehandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilehandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'epsilon_hist.data'"]}]}],"metadata":{"colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Car Racing DDQN.ipynb","provenance":[],"authorship_tag":"ABX9TyMuS3RwLfeD7aadel4YH0VM"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}